{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE9010: Introduction to Data Science\n",
    "## Semester 2 2017/18\n",
    "## Xavier Bresson\n",
    "## ⚠ Student name: \n",
    "<br>\n",
    "\n",
    "\n",
    "## Laboratory test 2: Supervised logistic regression\n",
    "Instruction: Check the box with the right answer.<br>\n",
    "\n",
    "Grading: You will receive 1 point for each of the questions you answer correctly.<br>\n",
    "$ $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# math library\n",
    "import numpy as np\n",
    "\n",
    "# visualization library\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scientific computing library\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# scikit-learn library\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# computational time\n",
    "import time\n",
    "\n",
    "# remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr>\n",
    "For the supervised logisitic regression task, we consider here $n$ training data $x_i, i=1,...,n$. Each data $x_i$ has d=2 features represented by $x=(x_{i(1)},x_{i(2)})$ and a label $y_i$.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data with numpy\n",
    "data = np.loadtxt('data/lab02_train_data.txt', delimiter=',')\n",
    "\n",
    "# plot\n",
    "x1 = data[:,0] # feature 1\n",
    "x2 = data[:,1] # feature 2\n",
    "idx_class0 = (data[:,2]==0) # index of class0\n",
    "idx_class1 = (data[:,2]==1) # index of class1\n",
    "\n",
    "plt.figure(1,figsize=(6,6))\n",
    "plt.scatter(x1[idx_class0], x2[idx_class0], s=60, c='r', marker='+', label='Class0') \n",
    "plt.scatter(x1[idx_class1], x2[idx_class1], s=30, c='b', marker='o', label='Class1')\n",
    "plt.title('Training data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: What are the mean value and the variance of the 1st data feature $x_{(1)}$?\n",
    "<hr>\n",
    "\n",
    "☐ mean=0.054, var=0.38 <br>\n",
    "☐ mean=-0.054, var=0.24 <br>\n",
    "☐ mean=0.054, var=0.24 <br>\n",
    "☐ mean=0.028, var=0.43 <br>\n",
    "\n",
    "Hint: You may use functions np.mean and np.var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Compute the mean value of the linear logistic regression function $f_w(x_i)$ applied to all training data $x_i,  i=1,...,n$ for $w=[1,1,1]$.\n",
    "<hr>\n",
    "\n",
    "☐ mean( $f(x_i)$ ) = 0.35<br>\n",
    "☐ mean( $f(x_i)$ ) = 0.57<br>\n",
    "☐ mean( $f(x_i)$ ) = 0.23<br>\n",
    "☐ mean( $f(x_i)$ ) = 0.75<br>\n",
    "\n",
    "\n",
    "Hints: \n",
    "\n",
    "1) Reminder of the linear logistic regression function:\n",
    "\n",
    "$$\n",
    "p_w(x)= \\sigma(Xw)=\n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "\\sigma(w_0 + w_1 x_{1(1)} + w_2 x_{1(2)}) \\\\ \n",
    "\\sigma(w_0 + w_1 x_{2(1)} + w_2 x_{2(2)}) \\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma(w_0 + w_1 x_{n(1)} + w_2 x_{n(2)})\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ with }\n",
    "\\quad\n",
    "X = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "1 & x_{1(1)} & x_{1(2)} \\\\ \n",
    "1 & x_{2(1)} & x_{2(2)} \\\\ \n",
    "\\vdots\\\\\n",
    "1 & x_{n(1)} & x_{n(2)} \n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ , }\n",
    "\\quad\n",
    "w = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2\n",
    "\\end{array} \n",
    "\\right],\n",
    "$$\n",
    "\n",
    "and the sigmoid/logistic function is $\\sigma(\\eta) = 1/(1+e^{-\\eta})$.\n",
    "\n",
    "2) Construct the $X$ matrix:\n",
    "$$\n",
    "X = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "1 & x_{1(1)} & x_{1(2)} \\\\ \n",
    "1 & x_{2(1)} & x_{2(2)} \\\\ \n",
    "\\vdots\\\\\n",
    "1 & x_{n(1)} & x_{n(2)} \n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "You may use the function `PolynomialFeatures` from sklearn library to automatically construct $X$. The function `PolynomialFeatures` generates a matrix consisting of **all polynomial combinations of the features** with degree less than or equal to a given value. Description of the function is given here:<br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html<br>\n",
    "\n",
    "Usage example: <br>\n",
    "$\\hspace{1cm}$ `from sklearn.preprocessing import PolynomialFeatures`<br>\n",
    "$\\hspace{1cm}$ `degree = 1 # degree=polynomial degree, degree=1 for linear features`<br>\n",
    "$\\hspace{1cm}$ `poly = PolynomialFeatures(degree)`<br>\n",
    "$\\hspace{1cm}$ `X = poly.fit_transform(data[:,0:2]) # input consists of the d data features`<br>\n",
    "        \n",
    "3) Compute the predictive function $f_w(x_i)$ for all training data $x_i,  i=1,...,n$ for $w=[1,1,1]$\n",
    "\n",
    "4) Compute the mean value of $f_w(x_i)$. You may use `np.mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data features\n",
    "degree = 1 # linear features\n",
    "poly = PolynomialFeatures(degree)\n",
    "X = poly.fit_transform(data[:,0:2])\n",
    "print(X.shape)\n",
    "\n",
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Compute the value of the linear logistic regression loss using all training data $x_i,  i=1,...,n$ and $w=[1,1,1]$?\n",
    "<hr>\n",
    "\n",
    "☐ loss = 0.63<br>\n",
    "☐ loss = 0.93<br>\n",
    "☐ loss = 0.45<br>\n",
    "☐ loss = 0.76<br>\n",
    "\n",
    "\n",
    "Hint: Reminder of the logistic regression loss:\n",
    "\n",
    "$$\n",
    "L(w)=-\\frac{1}{n} \\sum_{i=1}^n \\ \\Big(  y_i \\log(p_w(x_i)) + (1-y_i)\\log(1-p_w(x_i)) \\Big)\n",
    "$$\n",
    "\n",
    "The vectorized representation is:\n",
    "$$\n",
    "L(w)=-\\frac{1}{n} \\Big( y^T \\log(p_w(x)) + (1-y)^T \\log(1-p_w(x)) \\Big)\n",
    "$$\n",
    "with \n",
    "<br>\n",
    "$$\n",
    "p_w(x)= \\sigma(Xw)=\n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "\\sigma(w_0 + w_1 x_{1(1)} + w_2 x_{1(2)}) \\\\ \n",
    "\\sigma(w_0 + w_1 x_{2(1)} + w_2 x_{2(2)}) \\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma(w_0 + w_1 x_{n(1)} + w_2 x_{n(2)})\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ and }\n",
    "\\quad\n",
    "y = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function definition\n",
    "def loss_logreg(w,X,y): \n",
    "    n = len(y)\n",
    "    y_pred = f_pred(X,w)\n",
    "    loss = -1/n* ( y.T.dot(np.log(y_pred+1e-10)) + (1-y).T.dot(np.log(1-y_pred+1e-10)) ) \n",
    "    return loss\n",
    "\n",
    "y = data[:,2][:,None] # label\n",
    "\n",
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Compute the value of the linear logistic regression loss using all training data $x_i,  i=1,...,n$ and the   solution $w$ that optimizes the loss?\n",
    "<hr>\n",
    "\n",
    "☐ loss = 0.36<br>\n",
    "☐ loss = 0.69<br>\n",
    "☐ loss = 0.13<br>\n",
    "☐ loss = 0.87<br>\n",
    "\n",
    "\n",
    "Hints: \n",
    "\n",
    "1) The  solution $w$ that optimizes the logistic regression loss is defined as:\n",
    "\n",
    "$$\n",
    "\\min_w \\ L(w)=-\\frac{1}{n} \\Big( y^T \\log(p_w(x)) + (1-y)^T \\log(1-p_w(x)) \\Big) \\quad (1)\n",
    "$$\n",
    "\n",
    "2) You may use the function `minimize` from scipy library to automatically **find the $w$ solution that optimizes the logistic regression loss (1)**. The function `minimize` computes the minimizer $w$ of a function given some initial value `initial_w` and input arguments `args`. Description of the function is given here:<br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html<br>\n",
    "\n",
    "Usage example: <br>\n",
    "$\\hspace{1cm}$ `from scipy.optimize import minimize`<br>\n",
    "$\\hspace{1cm}$ `initial_w = np.zeros((X.shape[1],1)) # initial value`<br>\n",
    "$\\hspace{1cm}$ `result = minimize(loss_logreg, initial_w, args=(X,y), method='Powell', options={'maxiter':500})`<br>\n",
    "$\\hspace{1cm}$ `w_solution = np.array(result.x)[:,None]`<br>\n",
    "        \n",
    "3) Compute the loss value with the solution `w_solution`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "def plot_decision_boundary(X,w,data,title,cpt_fig):\n",
    "\n",
    "    # compute values p(x) for multiple data points x\n",
    "    x1_min, x1_max = X[:,1].min(), X[:,1].max() # min and max of grade 1\n",
    "    x2_min, x2_max = X[:,2].min(), X[:,2].max() # min and max of grade 2\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max)) # create meshgrid\n",
    "\n",
    "    n_mesh = xx1.reshape(-1).shape[0]\n",
    "    mesh_data = np.zeros((n_mesh,2))\n",
    "    mesh_data[:,0] = xx1.reshape(-1)\n",
    "    mesh_data[:,1] = xx2.reshape(-1)\n",
    "    mesh_X = poly.fit_transform(mesh_data)\n",
    "\n",
    "    p = f_pred(mesh_X,w)\n",
    "    p = p.reshape(xx1.shape)\n",
    "\n",
    "    # plot\n",
    "    x1 = data[:,0] # feature 1\n",
    "    x2 = data[:,1] # feature 2\n",
    "    idx_class0 = (data[:,2]==0) # index of class0\n",
    "    idx_class1 = (data[:,2]==1) # index of class1\n",
    "    plt.figure(cpt_fig,figsize=(6,6))\n",
    "    plt.scatter(x1[idx_class0], x2[idx_class0], s=60, c='r', marker='+', label='Class0') \n",
    "    plt.scatter(x1[idx_class1], x2[idx_class1], s=30, c='b', marker='o', label='Class1')\n",
    "    plt.contour(xx1, xx2, p, [0.5], linewidths=2, colors='k') \n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# plot decision boundary\n",
    "plot_decision_boundary(X,w_solution,data,'Decision boundary for linear prediction',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Compute the accuracy of the linear logistic regression model for the training data $x_i,  i=1,...,n$?\n",
    "<hr>\n",
    "\n",
    "☐ accuracy = 45.5%<br>\n",
    "☐ accuracy = 72.2%<br>\n",
    "☐ accuracy = 67.4%<br>\n",
    "☐ accuracy = 55.0%<br>\n",
    "\n",
    "\n",
    "Hint: You may use the function `compute_accuracy` given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def compute_accuracy(X,w,y):\n",
    "    \n",
    "    # continuous predictive function of the classes\n",
    "    pred = f_pred(X,w)\n",
    "    \n",
    "    # discrete predictive function of the classes\n",
    "    y_pred = (pred >= 0.5).astype('int').squeeze()\n",
    "\n",
    "    # accuracy\n",
    "    y = y.squeeze()\n",
    "    diff = (y_pred == y).astype('int')\n",
    "    accuracy = 100* sum(diff)/ y.shape[0]\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Compute the value of the logistic regression loss using the training data $x_i,  i=1,...,n$ with a polynomial degree=6?\n",
    "<hr>\n",
    "\n",
    "☐ loss = 0.27<br>\n",
    "☐ loss = 0.56<br>\n",
    "☐ loss = 0.18<br>\n",
    "☐ loss = 0.06<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data features\n",
    "degree = 6 \n",
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Compute the accuracy of the logistic regression model for the training data $x_i,  i=1,...,n$ with a polynomial degree=6?\n",
    "<hr>\n",
    "\n",
    "☐ accuracy = 58.7%<br>\n",
    "☐ accuracy = 65.2%<br>\n",
    "☐ accuracy = 86.4%<br>\n",
    "☐ accuracy = 74.0%<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# plot decision boundary\n",
    "plot_decision_boundary(X,w_solution,data,'Decision boundary for polynomial order 6',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Compute the value of the **regularized** logistic regression loss using all training data $x_i,  i=1,...,n$, the polynomial degree=6 and the regularization value $\\lambda$=0.1?\n",
    "<hr>\n",
    "\n",
    "☐ loss = 0.45<br>\n",
    "☐ loss = 0.34<br>\n",
    "☐ loss = 0.19<br>\n",
    "☐ loss = 0.24<br>\n",
    "\n",
    "\n",
    "Hints: \n",
    "\n",
    "1) Reminder of the logistic regression loss:\n",
    "\n",
    "$$\n",
    "L(w)=-\\frac{1}{n} \\Big( y^T \\log(p_w(x)) + (1-y)^T \\log(1-p_w(x)) \\Big)\n",
    "$$\n",
    "\n",
    "The **regularized** logistic regression loss is defined as:\n",
    "\n",
    "$$\n",
    "L(w)=-\\frac{1}{n} \\Big( y^T \\log(p_w(x)) + (1-y)^T \\log(1-p_w(x)) \\Big) + \\frac{\\lambda}{d} y^T y \\quad (2)\n",
    "$$\n",
    "\n",
    "\n",
    "2) Re-define the `loss_logreg` function in Q3 with the new definition (2).\n",
    "\n",
    "3) Change the input arguments of `minimize` function in Q4 to incorporate the regularization parameter $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Compute the accuracy of the regularized logistic regression model for all training data $x_i,  i=1,...,n$, the polynomial degree=6 and the regularization value $\\lambda$=0.1?\n",
    "<hr>\n",
    "\n",
    "☐ accuracy = 93.6%<br>\n",
    "☐ accuracy = 85.4%<br>\n",
    "☐ accuracy = 81.3%<br>\n",
    "☐ accuracy = 90.1%<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# plot decision boundary\n",
    "plot_decision_boundary(X,w_solution,data,'Decision boundary for polynomial order 6 and regularization',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Compute the accuracy of the validation set with the predicitive model learned on the training set with the polynomial degree=6 and for the regularization values $\\lambda$=0 (no regularization) and $\\lambda$=0.1?\n",
    "<hr>\n",
    "\n",
    "☐ validation accuracy for $\\lambda$=0/$\\lambda$=0.1 = 82.0%/93.0%<br>\n",
    "☐ validation accuracy for $\\lambda$=0/$\\lambda$=0.1 = 88.0%/81.0%<br>\n",
    "☐ validation accuracy for $\\lambda$=0/$\\lambda$=0.1 = 83.0%/90.0%<br>\n",
    "☐ validation accuracy for $\\lambda$=0/$\\lambda$=0.1 = 87.0%/84.0%<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import validation data with numpy\n",
    "val_data = np.loadtxt('data/lab02_val_data.txt', delimiter=',')\n",
    "\n",
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
