{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# math library\n",
    "import numpy as np\n",
    "\n",
    "# visualization library\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 3d visualization\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# computational time\n",
    "import time\n",
    "\n",
    "data = np.loadtxt('data/lab01_data2.txt', delimiter=',')\n",
    "n = (data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 3)\n",
      "[[0.46985 0.6     0.57137]\n",
      " [0.3573  0.6     0.47135]\n",
      " [0.53595 0.6     0.52722]\n",
      " [0.31621 0.4     0.33148]\n",
      " [0.66994 0.8     0.7714 ]]\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data[:5,:])\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 3)\n",
      "[[1.      0.46985 0.6    ]\n",
      " [1.      0.3573  0.6    ]\n",
      " [1.      0.53595 0.6    ]\n",
      " [1.      0.31621 0.4    ]\n",
      " [1.      0.66994 0.8    ]]\n",
      "(3, 1)\n",
      "[[0.51800721]\n",
      " [0.52923661]\n",
      " [0.51140302]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    sigmoid_f = 1 / (1 + np.exp(-z)) #YOUR CODE HERE\n",
    "    return sigmoid_f\n",
    "\n",
    "X = np.ones([n,3]) \n",
    "X[:,1:3] = data[:,0:2]\n",
    "print(X.shape)\n",
    "print(X[:5,:])\n",
    "\n",
    "\n",
    "# parameters vector\n",
    "w = np.array([0.2,-0.4,0.1])[:,None] # [:,None] adds a singleton dimension\n",
    "print(w.shape)\n",
    "\n",
    "\n",
    "# predictive function definition\n",
    "def f_pred(X,w): \n",
    "    p = sigmoid(X.dot(w)) #YOUR CODE HERE\n",
    "    return p\n",
    "\n",
    "\n",
    "# Test predicitive function \n",
    "y_pred = f_pred(X,w)\n",
    "print(y_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of admission is [0.18]\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = (1 / (1 + np.exp(-47)))*np.array([[1,0.2,0.6]]).dot(w_init)\n",
    "print('Probability of admission is',y_pred2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 1)\n",
      "[[0.70512913]]\n"
     ]
    }
   ],
   "source": [
    "def loss_logreg(y_pred,y): \n",
    "    n = len(y)\n",
    "    loss = -1/n* ( y.T.dot(np.log(y_pred)) + (1-y).T.dot(np.log(1-y_pred)) ) #YOUR CODE HERE\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Test loss function \n",
    "y = data[:,2][:,None] # label \n",
    "print(y.shape)\n",
    "#print(y)\n",
    "y_pred = f_pred(X,w) # prediction\n",
    "loss = loss_logreg(y_pred,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06954591]\n",
      " [-0.02740689]\n",
      " [ 0.01873215]]\n"
     ]
    }
   ],
   "source": [
    "# gradient function definition\n",
    "def grad_loss(y_pred,y,X):\n",
    "    n = len(y)\n",
    "    grad = 2/n* X.T.dot(y_pred-y) #YOUR CODE HERE\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Test grad function \n",
    "y_pred = f_pred(X,w)\n",
    "grad = grad_loss(y_pred,y,X)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-eb4a5d7a0425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#w_init = np.array([0,0,0])[:,None]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_iters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-eb4a5d7a0425>\u001b[0m in \u001b[0;36mgrad_desc\u001b[0;34m(X, y, w_init, tau, max_iter)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgrad_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gradient of the loss #YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mgrad_f\u001b[0m \u001b[0;31m# update rule of gradient descent #YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mL_iters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save the current loss value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mw_iters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# save the current w value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "def loss_logreg2(y_pred,y, X, w): \n",
    "    n = len(y)\n",
    "    loss = 1/n* np.abs(f_pred(X,w)-y) #YOUR CODE HERE\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grad_desc(X, y , w_init=np.array([0,0,0])[:,None] ,tau=1e-4, max_iter=500):\n",
    "\n",
    "    L_iters = np.zeros([max_iter]) # record the loss values\n",
    "    w_iters = np.zeros([max_iter,2]) # record the loss values\n",
    "    w = w_init # initialization\n",
    "    for i in range(max_iter): # loop over the iterations\n",
    "        y_pred = f_pred(X,w) # linear predicition function  #YOUR CODE HERE\n",
    "        grad_f = grad_loss(y_pred,y,X) # gradient of the loss #YOUR CODE HERE\n",
    "        w = w - tau* grad_f # update rule of gradient descent #YOUR CODE HERE\n",
    "        L_iters[i] = 1/n* np.abs(f_pred(X,w)-y) # save the current loss value \n",
    "        w_iters[i,:] = w[0],w[1] # save the current w value \n",
    "        \n",
    "    return w, L_iters, w_iters\n",
    "\n",
    "\n",
    "# run gradient descent algorithm\n",
    "start = time.time()\n",
    "w_init = np.array([0.2,-0.4,0.1])[:,None]\n",
    "#w_init = np.array([0,0,0])[:,None]\n",
    "tau = 0.1; max_iter = 200\n",
    "w, L_iters, w_iters = grad_desc(X,y,w_init,tau,max_iter)\n",
    "print('Time=',time.time() - start)\n",
    "print(L_iters[-1])\n",
    "print(w)\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(3)\n",
    "plt.plot(np.array(range(max_iter)), L_iters)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time= 0.013224124908447266\n",
      "0.6809963625600635\n",
      "[[-0.32435276]\n",
      " [ 0.40376063]\n",
      " [ 0.20498445]]\n"
     ]
    }
   ],
   "source": [
    "def grad_desc2(X, y , w_init=np.array([0,0,0])[:,None] ,tau=1e-4, max_iter=500):\n",
    "\n",
    "    L_iters = np.zeros([max_iter]) # record the loss values\n",
    "    w_iters = np.zeros([max_iter,2]) # record the loss values\n",
    "    w = w_init # initialization\n",
    "    for i in range(max_iter): # loop over the iterations\n",
    "        y_pred = f_pred(X,w) # linear predicition function  #YOUR CODE HERE\n",
    "        grad_f = grad_loss(y_pred,y,X) # gradient of the loss #YOUR CODE HERE\n",
    "        w = w - tau* grad_f # update rule of gradient descent #YOUR CODE HERE\n",
    "        L_iters[i] = loss_logreg(y_pred,y) # save the current loss value \n",
    "        w_iters[i,:] = w[0],w[1] # save the current w value \n",
    "        \n",
    "    return w, L_iters, w_iters\n",
    "\n",
    "start = time.time()\n",
    "w_init = np.array([0.2,-0.4,0.1])[:,None]\n",
    "#w_init = np.array([0,0,0])[:,None]\n",
    "tau = 0.1; max_iter = 200\n",
    "w, L_iters, w_iters = grad_desc(X,y,w_init,tau,max_iter)\n",
    "print('Time=',time.time() - start)\n",
    "print(L_iters[-1])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18]]\n"
     ]
    }
   ],
   "source": [
    "print((1 / (1 + np.exp(-47)))*np.array([[1,0.2,0.6]]).dot(w_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
